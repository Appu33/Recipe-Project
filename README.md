Recipeproject

ğŸ½ï¸ Firebase Recipe Analytics Pipeline

A Complete ETL + Analytics Project Using Firebase Firestore & Python

ğŸ“Œ Project Overview

This project implements a complete data engineering pipeline that:

Extracts recipe, user, and interaction data from Firebase Firestore

Transforms, validates, and normalizes the data using Python

Loads clean structured data into CSV datasets

Generates visual and statistical analytics insights

This project demonstrates:

ğŸ”¥ Firebase Database Integration

ğŸ§± Proper Data Modeling

ğŸ”„ ETL Pipeline (Extract â†’ Transform â†’ Load)

âœ” Data Quality Validation

ğŸ“Š Analytics & Charts

ğŸ“ Production-ready folder structure

ğŸ“ Industry-grade documentation

ğŸ“‚ Project Structure
Recipeproject/
â”‚â”€â”€ README.md
â”‚â”€â”€ admin_key.json
â”‚â”€â”€ firebase_config.py
â”‚â”€â”€ etl_pipeline.py
â”‚â”€â”€ validate_data.py
â”‚â”€â”€ analytics.py
â”‚â”€â”€ seed_recipes.py
â”‚â”€â”€ seed_users.py
â”‚â”€â”€ seed_interactions.py
â”‚â”€â”€ recipe.csv
â”‚â”€â”€ ingredients.csv
â”‚â”€â”€ interactions.csv
â”‚â”€â”€ steps.csv
â”‚â”€â”€ screenshots/
â””â”€â”€ __pycache__/

ğŸ§± Data Model

Your pipeline produces four normalized datasets.

ğŸ“˜ recipe.csv
Column	Description
id	Recipe ID
title	Recipe name
prep_time	Preparation time (min)
difficulty	easy / medium / hard
created_by	User ID (FK â†’ users)
ğŸ¥— ingredients.csv
Column	Description
recipe_id	FK â†’ recipe.id
name	Ingredient name
ğŸ“„ steps.csv
Column	Description
recipe_id	FK â†’ recipe.id
step_no	Step number
description	Step instructions
â­ interactions.csv
Column	Description
user_id	FK â†’ users
recipe_id	FK â†’ recipe
viewed	1 = viewed
liked	1 = liked
cooked	1 = attempted cook
ğŸ”„ ETL Pipeline (etl_pipeline.py)
1ï¸âƒ£ Extract

Connects to Firestore using firebase_config.py

Fetches collections:

recipes

users

interactions

Saves raw data to CSV

2ï¸âƒ£ Transform

Cleans and standardizes:

Removes duplicates

Converts timestamps to date

Standardizes difficulty labels

Converts viewed/liked/cooked â†’ 0/1

Removes blank or malformed rows

3ï¸âƒ£ Validate (validate_data.py)

Quality checks performed:

Missing or null values

Broken foreign keys

Invalid difficulty fields

Incorrect datatypes

Empty ingredient/step lists

4ï¸âƒ£ Load

Outputs final normalized CSV files:

recipe.csv

ingredients.csv

steps.csv

interactions.csv

ğŸ“Š Analytics (analytics.py)

Run:

python analytics.py


Generates insights such as:

âœ” Most common ingredients

âœ” Average preparation time

âœ” Recipe difficulty distribution

âœ” Top viewed recipes

âœ” Top liked recipes

âœ” Correlation: prep_time vs likes

âœ” User engagement metrics

Visual charts are automatically saved inside:

screenshots/

ğŸ“ˆ Example Insights
Insight	Example Output
Average prep time	22 minutes
Most common ingredient	Salt
Most liked recipe	r001
Most viewed recipe	r001
Strongest correlation	prep_time vs likes

ğŸ§ª How to Run This Project
1ï¸âƒ£ Install dependencies
pip install pandas matplotlib firebase-admin seaborn plotly


If permission issues occur:

pip install --user pandas matplotlib seaborn plotly

2ï¸âƒ£ Add Firebase Admin Key

Place admin_key.json inside the project folder.

3ï¸âƒ£ Run ETL
python etl_pipeline.py

4ï¸âƒ£ Run Data Validation (optional)
python validate_data.py

5ï¸âƒ£ Run Analytics
python analytics.py

âš ï¸ Known Limitations

Firestore is not optimized for heavy relational workloads

ETL runs in batch mode, not real-time

CSV export cannot store nested JSON perfectly

Analytics limited to available interaction types (no ratings/comments)

ğŸš€ Future Enhancements

Convert this pipeline into an Apache Airflow DAG

Store cleaned data in BigQuery instead of CSV

Build a full Power BI / Tableau dashboard

Add:

recipe ratings

comments

user segmentation

Add real-time streaming using Firebase triggers

Dockerize the entire pipeline

ğŸ”š Conclusion

This project delivers a complete end-to-end ETL and analytics pipeline built on Firebase and Python. It converts raw Firestore data into clean, validated CSV datasets and generates meaningful insights with visual charts. The structure is modular, easy to extend, and serves as a strong foundation for real-world data engineering workflows.
